# ML-et-AI-project-with-3-datasets
Ce projet en 3 parties est destiné à nous familiariser avec Machine Learning (ML). Les 3 parties sont comme suit:  Dans la première partie, nous avons implémente un algorithme de sélection d'attribut. Étant donné un ensemble de données de 𝑚 attributs, l’algorithme calcule simplement le rapport de gain de chacun des attributs et le conserve haut ⌈𝑚⌉ les attributs. Cette partie devrait être mise en œuvre sur le jeu de données d’échecs d’Alen Shapiro.1 Dans ce ensemble de données, il y a 36 attributs, nos algorithmes ont donc choisir les 4 avec le gain le plus élevé Ratio et stockez le jeu de données résultant (avec seulement ces 4 attributs) dans un fichier séparé.  Dans la deuxième partie, nous avons implémente l’algorithme de k-NN le plus proche pour la classification.  En utilisant la Distance euclidienne et k = 1 et nous avons appliquer notre  algorithme au Wisconsin pour le cancer du sein (diagnostic). Cependant, avant de mettre en œuvre l’algorithme, nous avons divisez nos données en un ensemble d’apprentissage et en un ensemble de test. L’ensemble d’entraînement comprend 90% des premiers cas, alors que l’ensemble de test comprend des 10% restants. notre algorithme doit stocker ses prédictions dans un fichier séparé et afficher la précision de ces prédictions. Dans la dernière partie, nous avons  implémente une technique de clustering simple qui utilise deux versions de jeux de données du Diabète, une version discrétisée et une version non discrétisée (d’origine). Plus précisément, nous utiliserons le jeu de données sur le diabète Indien Pima discrétisé par mangrove.   Le jeu de données a de nombreux attributs, mais nous  nous concentrerons que sur 5 attributs non discrétisés (âge, IMC, glucose, insuline, grossesses) et 5 discrétisées (LabelPAge, LabelPBMI, LabelPGlucose, LabelPInsulin, Labelpgrossesses). Ainsi la première chose à faire est de supprimer tout sauf ces 10 attributs. L’algorithme commence par calculer de la corrélation entre chaque paire d’attributs non discrétisés et choisit le pair avec la corrélation la plus faible (c.-à-d., avec le coefficient de corrélation le plus proche de 0). Appelons cette paire AX et Ay. Ensuite, pour ces deux attributs, il crée un cluster pour chaque combinaison possible de valeurs pour les versions discrétisées de AX et AY.  Par exemple, disons que la version discrétisée de la hache a les valeurs haute et basse et la version discrétisée d’ay a les valeurs grandes et petites. Alors Il y aura les 4 clusters suivants: C1: avec des enregistrements contenant les valeurs haute et grande pour AX et AY, respectivement. C2: avec des enregistrements contenant les valeurs haute et petite pour AX et AY, respectivement. C3: avec des enregistrements contenant les valeurs basses et grandes pour AX et AY, respectivement. C4: avec des enregistrements contenant les valeurs basses et petites pour AX et AY, respectivement. Notre algorithme a du créer un fichier distinct contenant les enregistrements de chaque cluster. Elle  a également évaluer le regroupement résultant en calculant la distance euclidienne maximale entre deux enregistrements dans le même cluster et la distance euclidienne minimale entre deux enregistrements dans différents clusters. Notez que ces distances doivent être calculées en fonction des 5 attributs non discrétisés.
