# ML-et-AI-project-with-3-datasets
Ce projet en 3 parties est destinÃ© Ã  nous familiariser avec Machine Learning (ML). Les 3 parties sont comme suit:  Dans la premiÃ¨re partie, nous avons implÃ©mente un algorithme de sÃ©lection d'attribut. Ã‰tant donnÃ© un ensemble de donnÃ©es de ğ‘š attributs, lâ€™algorithme calcule simplement le rapport de gain de chacun des attributs et le conserve haut âŒˆğ‘šâŒ‰ les attributs. Cette partie devrait Ãªtre mise en Å“uvre sur le jeu de donnÃ©es dâ€™Ã©checs dâ€™Alen Shapiro.1 Dans ce ensemble de donnÃ©es, il y a 36 attributs, nos algorithmes ont donc choisir les 4 avec le gain le plus Ã©levÃ© Ratio et stockez le jeu de donnÃ©es rÃ©sultant (avec seulement ces 4 attributs) dans un fichier sÃ©parÃ©.  Dans la deuxiÃ¨me partie, nous avons implÃ©mente lâ€™algorithme de k-NN le plus proche pour la classification.  En utilisant la Distance euclidienne et k = 1 et nous avons appliquer notre  algorithme au Wisconsin pour le cancer du sein (diagnostic). Cependant, avant de mettre en Å“uvre lâ€™algorithme, nous avons divisez nos donnÃ©es en un ensemble dâ€™apprentissage et en un ensemble de test. Lâ€™ensemble dâ€™entraÃ®nement comprend 90% des premiers cas, alors que lâ€™ensemble de test comprend des 10% restants. notre algorithme doit stocker ses prÃ©dictions dans un fichier sÃ©parÃ© et afficher la prÃ©cision de ces prÃ©dictions. Dans la derniÃ¨re partie, nous avons  implÃ©mente une technique de clustering simple qui utilise deux versions de jeux de donnÃ©es du DiabÃ¨te, une version discrÃ©tisÃ©e et une version non discrÃ©tisÃ©e (dâ€™origine). Plus prÃ©cisÃ©ment, nous utiliserons le jeu de donnÃ©es sur le diabÃ¨te Indien Pima discrÃ©tisÃ© par mangrove.   Le jeu de donnÃ©es a de nombreux attributs, mais nous  nous concentrerons que sur 5 attributs non discrÃ©tisÃ©s (Ã¢ge, IMC, glucose, insuline, grossesses) et 5 discrÃ©tisÃ©es (LabelPAge, LabelPBMI, LabelPGlucose, LabelPInsulin, Labelpgrossesses). Ainsi la premiÃ¨re chose Ã  faire est de supprimer tout sauf ces 10 attributs. Lâ€™algorithme commence par calculer de la corrÃ©lation entre chaque paire dâ€™attributs non discrÃ©tisÃ©s et choisit le pair avec la corrÃ©lation la plus faible (c.-Ã -d., avec le coefficient de corrÃ©lation le plus proche de 0). Appelons cette paire AX et Ay. Ensuite, pour ces deux attributs, il crÃ©e un cluster pour chaque combinaison possible de valeurs pour les versions discrÃ©tisÃ©es de AX et AY.  Par exemple, disons que la version discrÃ©tisÃ©e de la hache a les valeurs haute et basse et la version discrÃ©tisÃ©e dâ€™ay a les valeurs grandes et petites. Alors Il y aura les 4 clusters suivants: C1: avec des enregistrements contenant les valeurs haute et grande pour AX et AY, respectivement. C2: avec des enregistrements contenant les valeurs haute et petite pour AX et AY, respectivement. C3: avec des enregistrements contenant les valeurs basses et grandes pour AX et AY, respectivement. C4: avec des enregistrements contenant les valeurs basses et petites pour AX et AY, respectivement. Notre algorithme a du crÃ©er un fichier distinct contenant les enregistrements de chaque cluster. Elle  a Ã©galement Ã©valuer le regroupement rÃ©sultant en calculant la distance euclidienne maximale entre deux enregistrements dans le mÃªme cluster et la distance euclidienne minimale entre deux enregistrements dans diffÃ©rents clusters. Notez que ces distances doivent Ãªtre calculÃ©es en fonction des 5 attributs non discrÃ©tisÃ©s.
